{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval as leval\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def get_outliers(series):\n",
    "    q1 = series.quantile(0.25)\n",
    "    q3 = series.quantile(0.75)\n",
    "    iqr = q3-q1 #Interquartile range\n",
    "    fence_low  = q1-1.5*iqr\n",
    "    fence_high = q3+1.5*iqr\n",
    "    outliers = series[(series < fence_low) | (series > fence_high)]\n",
    "    return outliers\n",
    "\n",
    "from emoji import UNICODE_EMOJI\n",
    "\n",
    "def is_emoji(s):\n",
    "    flags = re.findall(u'[\\U0001F1E6-\\U0001F1FF]', s)\n",
    "    if flags:\n",
    "        return True\n",
    "    return s in UNICODE_EMOJI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get WTR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_text_df = pd.read_csv('text_extraction/reference_html_as_sentences_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. References\n",
    "\n",
    "In here we have the information about the references, including the reference ids and the text/sentences extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_text_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def check_column_dist(df, col):\n",
    "    counts = df[col].value_counts().reset_index()\n",
    "    counts.columns = [col, 'counts']\n",
    "    counts['per'] = 100*counts['counts']/counts['counts'].sum()\n",
    "    counts[col] = counts[col].astype(str)\n",
    "    return counts\n",
    "\n",
    "# Netlocs are equaly spread\n",
    "# codes are all good\n",
    "# reasons are good\n",
    "fig, ax = plt.subplots(1,3,figsize=(15,10))\n",
    "sns.barplot(data=check_column_dist(reference_text_df, 'netloc_agg'), x='per', y='netloc_agg', ax=ax[0])\n",
    "sns.barplot(data=check_column_dist(reference_text_df, 'code'), x='per', y='code', ax=ax[1])\n",
    "sns.barplot(data=check_column_dist(reference_text_df, 'reason'), x='per', y='reason', ax=ax[2])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Claims\n",
    "\n",
    "This has all the claim data, including labels, aliases, descriptions, and language IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_data_df = pd.read_csv('text_extraction/text_reference_claims_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total counts:')\n",
    "print(f'{claim_data_df.claim_id.unique().shape[0]} unique claims')\n",
    "print(f'{claim_data_df.reference_id.unique().shape[0]} unique references')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considerable amount of aliases for entities, LOTS for properties, reasonable for objects.\n",
    "# We can use this to generate multiple verbalisations based on aliases.\n",
    "fig, ax = plt.subplots(4,3,figsize=(15,10))\n",
    "sns.barplot(data=check_column_dist(claim_data_df, 'rank'), x='per', y='rank', ax=ax[0][0])\n",
    "sns.barplot(data=check_column_dist(claim_data_df, 'datatype'), x='per', y='datatype', ax=ax[0][1])\n",
    "#sns.barplot(data=check_column_dist(claim_data_df, 'entity_label_lan'), x='per', y='entity_label_lan', ax=ax[0][2])\n",
    "sns.barplot(data=check_column_dist(claim_data_df, 'entity_label_lan'), x='per', y='entity_label_lan', ax=ax[1][0])\n",
    "sns.barplot(data=check_column_dist(claim_data_df, 'entity_alias_lan'), x='per', y='entity_alias_lan', ax=ax[1][1])\n",
    "sns.barplot(data=check_column_dist(claim_data_df, 'entity_desc_lan'), x='per', y='entity_desc_lan', ax=ax[1][2])\n",
    "sns.barplot(data=check_column_dist(claim_data_df, 'property_label_lan'), x='per', y='property_label_lan', ax=ax[2][0])\n",
    "sns.barplot(data=check_column_dist(claim_data_df, 'property_alias_lan'), x='per', y='property_alias_lan', ax=ax[2][1])\n",
    "sns.barplot(data=check_column_dist(claim_data_df, 'property_desc_lan'), x='per', y='property_desc_lan', ax=ax[2][2])\n",
    "sns.barplot(data=check_column_dist(claim_data_df, 'object_label_lan'), x='per', y='object_label_lan', ax=ax[3][0])\n",
    "sns.barplot(data=check_column_dist(claim_data_df, 'object_alias_lan'), x='per', y='object_alias_lan', ax=ax[3][1])\n",
    "sns.barplot(data=check_column_dist(claim_data_df, 'object_desc_lan'), x='per', y='object_desc_lan', ax=ax[3][2])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of entities and properties involved in the claim data\n",
    "\n",
    "fig, ax = plt.subplots(4,2,figsize=(10,12))\n",
    "\n",
    "ax[0][0].set_yscale('log')\n",
    "entity_count = claim_data_df.entity_id.value_counts()\n",
    "ax[0][0].set_title('Boxplot of Entity distribution')\n",
    "sns.boxplot(data = entity_count, ax=ax[0][0])\n",
    "ax[0][1].set_title('KDE of Entity distribution')\n",
    "sns.kdeplot(data = entity_count, ax=ax[0][1])\n",
    "\n",
    "ax[1][0].set_yscale('log')\n",
    "property_count = claim_data_df.property_id.value_counts()\n",
    "ax[1][0].set_title('Boxplot of Property distribution')\n",
    "sns.boxplot(data = property_count, ax=ax[1][0])\n",
    "ax[1][1].set_title('KDE of Property distribution')\n",
    "sns.kdeplot(data = property_count, ax=ax[1][1])\n",
    "\n",
    "datatype_count = claim_data_df.datatype.value_counts().reset_index()\n",
    "ax[2][0].set_title('Barplot of Datatype distribution')\n",
    "sns.barplot(data = datatype_count, ax=ax[2][0], x='index', y='datatype')\n",
    "\n",
    "entity_alias_count = claim_data_df.entity_alias.apply(lambda x : len(leval(x)) if x != 'no-alias' else 0)\n",
    "ax[2][1].set_title('KDE of Entity alias count distribution')\n",
    "sns.kdeplot(data = entity_alias_count, ax=ax[2][1])\n",
    "\n",
    "entity_alias_count = claim_data_df.property_alias.apply(lambda x : len(leval(x)) if x != 'no-alias' else 0)\n",
    "ax[3][0].set_title('KDE of Property alias count distribution')\n",
    "sns.kdeplot(data = entity_alias_count, ax=ax[3][0])\n",
    "\n",
    "entity_alias_count = claim_data_df.object_alias.apply(lambda x : len(leval(x)) if x != 'no-alias' else 0)\n",
    "ax[3][1].set_title('KDE of Object alias count distribution')\n",
    "sns.kdeplot(data = entity_alias_count, ax=ax[3][1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# We can see that entities and properties are somewhat fairly spread, with most within a lesser volume and a few outliers.\n",
    "# Datatype has an OK distribution, nothing wrong there for this analysis.\n",
    "# KDE of alias counts for ent, prop, and obj show most cases on low count and a few outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Verbalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verbalisation import verbalisation_module\n",
    "\n",
    "# If updating the module\n",
    "#from importlib import reload\n",
    "#reload(verbalisation_module)\n",
    "\n",
    "verb_module = verbalisation_module.VerbModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available(),\\\n",
    "torch.cuda.device_count(),\\\n",
    "torch.cuda.current_device(),\\\n",
    "torch.cuda.device(0),\\\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = verb_module.verbalise('translate Graph to English: <H> World Trade Center <R> height <T> 200 meter <H> World Trade Center <R> is a <T> tower')\n",
    "#print(verbs)\n",
    "assert verbs == 'The World Trade Center is a tower with a height of 200 meters.'\n",
    "\n",
    "verbs = verb_module.verbalise({'subject': 'World Trade Center','predicate': 'height','object' : '200 meter'})\n",
    "#print(verbs)\n",
    "assert verbs == 'The height of the World Trade Center is 200 meters.'\n",
    "\n",
    "verbs = verb_module.verbalise([[\n",
    "        {'subject': 'World Trade Center','predicate': 'is a','object' : 'tower'},\n",
    "        {'subject': 'World Trade Center','predicate': 'height','object' : '200 meter'}\n",
    "]])\n",
    "#print(verbs)\n",
    "assert verbs == 'The World Trade Center is a tower with a height of 200 meters.'\n",
    "\n",
    "verbs = verb_module.verbalise([\n",
    "    {'subject': 'World Trade Center','predicate': 'is a','object' : 'tower'},\n",
    "    {'subject': 'World Trade Center','predicate': 'height','object' : '200 meter'}\n",
    "])\n",
    "#print(verbs)\n",
    "assert verbs == [\n",
    "    'The World Trade Center is a tower.',\n",
    "    'The height of the World Trade Center is 200 meters.'\n",
    "]\n",
    "\n",
    "verbs = verb_module.verbalise({'subject': 'Cuhppulčohkka','predicate': 'instance of','object' : 'native label'})\n",
    "#print(verbs)\n",
    "assert verbs == 'Cuhppul<unk>ohkka is an instance of a native label.'\n",
    "\n",
    "verb_module.add_label_to_unk_replacer('Cuhppulčohkka')\n",
    "sent = verb_module.replace_unks_on_sentence('Cuhppul<unk>ohkka is a native label.', empty_after=True)\n",
    "#print(verbs)\n",
    "assert sent == 'Cuhppulčohkka is a native label.'\n",
    "\n",
    "verb_module.add_label_to_unk_replacer('Cuhppulohkkač')\n",
    "sent = verb_module.replace_unks_on_sentence('Cuhppulohkka<unk> is a native label.', empty_after=True)\n",
    "#print(verbs)\n",
    "assert sent == 'Cuhppulohkkač is a native label.'\n",
    "\n",
    "\n",
    "verb_module.add_label_to_unk_replacer('🇬🇧')\n",
    "sent = verb_module.replace_unks_on_sentence('The Value Added Tax Order 2016 applies to jurisdiction of <unk>.', empty_after=True)\n",
    "#print(verbs)\n",
    "assert sent == 'The Value Added Tax Order 2016 applies to jurisdiction of 🇬🇧.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    verbalised_claims_df = pd.read_csv('verbalisation/verbalised_claims_df.csv')\n",
    "except Exception:\n",
    "    verbalised_claims_df = None\n",
    "verbalised_claims_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "verbalised_claims_this_batch = []\n",
    "\n",
    "claim_data_to_keep = [\n",
    "    'reference_id', 'entity_id', 'claim_id', 'rank', 'property_id', 'datatype',\n",
    "    'entity_label', 'entity_desc', 'property_label', 'property_desc', 'object_label', 'object_desc'\n",
    "] # also add entity_label_is_alias, same for property and object\n",
    "\n",
    "if verbalised_claims_df is not None and not verbalised_claims_df.empty:\n",
    "    verbalised_claims = json.loads(\n",
    "        verbalised_claims_df[verbalised_claims_df['verbalisation'] != 'NO_VERBALISATION'].to_json(orient=\"records\")\n",
    "    )\n",
    "else:\n",
    "    verbalised_claims = []\n",
    "\n",
    "print(len(verbalised_claims))\n",
    "\n",
    "with open('verbalisation.log','w+',encoding='utf-8') as f:\n",
    "\n",
    "    for i, row in tqdm(claim_data_df.iterrows(), total=claim_data_df.shape[0]):\n",
    "    \n",
    "        try:\n",
    "\n",
    "            subjects = [row['entity_label']] if row['entity_label_lan'] == 'en' else []\n",
    "            subjects += leval(row['entity_alias']) if row['entity_alias_lan'] == 'en' else []\n",
    "\n",
    "            for i_s, subject in enumerate(subjects):\n",
    "\n",
    "                predicates = [row['property_label']] if row['property_label_lan'] == 'en' else []\n",
    "                predicates += leval(row['property_alias']) if row['property_alias_lan'] == 'en' else []\n",
    "\n",
    "                for i_p, predicate in enumerate(predicates):\n",
    "\n",
    "                    objects = [row['object_label']] if row['object_label_lan'] == 'en' else []\n",
    "                    objects += leval(row['object_alias']) if row['object_alias_lan'] == 'en' else []\n",
    "\n",
    "                    for i_o, object_ in enumerate(objects):\n",
    "\n",
    "                        verbalised_claim_entry = {\n",
    "                            c : row[c] for c in claim_data_to_keep\n",
    "                        }                        \n",
    "                        \n",
    "                        if type(verbalised_claims_df) == pd.core.frame.DataFrame:\n",
    "                            if not verbalised_claims_df[\n",
    "                                (verbalised_claims_df['reference_id'] == row['reference_id']) &\\\n",
    "                                (verbalised_claims_df['claim_id'] == row['claim_id']) &\\\n",
    "                                (verbalised_claims_df['entity_label'] == subject) &\\\n",
    "                                (verbalised_claims_df['object_label'] == object_) &\\\n",
    "                                (verbalised_claims_df['property_label'] == predicate)                        \n",
    "                            ].empty:\n",
    "                                continue                        \n",
    "                        #print('nonempty found:','-'.join([subject, predicate, object_]))\n",
    "\n",
    "                        verbalised_claim_entry.update({\n",
    "                            'entity_label_is_alias': (i_s != 0),\n",
    "                            'property_label_is_alias': (i_p != 0),\n",
    "                            'object_label_is_alias': (i_o != 0),\n",
    "                            'entity_label': subject,\n",
    "                            'property_label': predicate,\n",
    "                            'object_label': object_\n",
    "                        })\n",
    "                        \n",
    "                        #print(f\"[{i}/{claim_data_df.shape[0]-1}, {i_s}/{len(subjects)-1}, {i_p}/{len(predicates)-1}, {i_o}/{len(objects)-1}]\")\n",
    "                        is_last =\\\n",
    "                            (i == claim_data_df.shape[0]-1) &\\\n",
    "                            (i_s == len(subjects)-1) &\\\n",
    "                            (i_p == len(predicates)-1) &\\\n",
    "                            (i_o == len(objects)-1)\n",
    "                        #if is_last:\n",
    "                        #    print('LAST')\n",
    "                            \n",
    "                        \n",
    "                        verbalised_claims_this_batch.append(verbalised_claim_entry)\n",
    "                        if len(verbalised_claims_this_batch) >= BATCH_SIZE or is_last:\n",
    "                            #print('verbalising...')\n",
    "                            verbalisation_inputs = [{\n",
    "                                'subject':e['entity_label'], 'predicate':e['property_label'], 'object':e['object_label']\n",
    "                            } for e in verbalised_claims_this_batch]\n",
    "\n",
    "                            try:\n",
    "                                #verbalisations = verb_module.verbalise(verbalisation_inputs)\n",
    "                                verbalisations = ['TEST' for _ in verbalisation_inputs]\n",
    "                                #f.write(\n",
    "                                #    f'Verbalising: {[(v[\"reference_id\"], v['claim_id']) for v in verbalised_claims_this_batch]}\\n'\n",
    "                                #)\n",
    "                            except Exception:\n",
    "                                print('ERROR VERBALISING:', verbalisation_inputs, sep='\\n')\n",
    "                                raise            \n",
    "\n",
    "                            for j in range(len(verbalised_claims_this_batch)):\n",
    "                                try:\n",
    "                                    verb_module.add_label_to_unk_replacer(verbalised_claims_this_batch[j]['entity_label'])\n",
    "                                    verb_module.add_label_to_unk_replacer(verbalised_claims_this_batch[j]['object_label'])\n",
    "                                    verbalised_claims_this_batch[j].update({\n",
    "                                        'verbalisation' : verbalisations[j],\n",
    "                                        'verbalisation_unks_replaced': verb_module.replace_unks_on_sentence(\n",
    "                                            verbalisations[j], empty_after=True\n",
    "                                        )\n",
    "                                    })\n",
    "                                except Exception:\n",
    "                                    print('REPLACING_ERROR on', verbalised_claims_this_batch[j])\n",
    "                                    verbalised_claims_this_batch[j].update({\n",
    "                                        'verbalisation' : verbalisations[j],\n",
    "                                        'verbalisation_unks_replaced': 'REPLACING_ERROR'\n",
    "                                    })\n",
    "\n",
    "                            verbalised_claims += verbalised_claims_this_batch\n",
    "                            verbalised_claims_this_batch = []\n",
    "\n",
    "        except Exception:\n",
    "            print(row)\n",
    "            #pprint(verbalised_claims_this_row)\n",
    "            raise\n",
    "\n",
    "# Load \n",
    "verbalised_claims_df = pd.DataFrame(verbalised_claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# REMOVING REMAINING <UNK> TOKENS\n",
    "verbalised_claims_df['verbalisation_unks_replaced_then_dropped'] = None\n",
    "for i, row in tqdm(verbalised_claims_df.iterrows(), total=verbalised_claims_df.shape[0]):\n",
    "    s = row['verbalisation_unks_replaced']\n",
    "    # Removing remaining <unk> tokens\n",
    "    s = re.sub('<unk>', '', s)\n",
    "    # Removing doublespaces\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    # Removing spaces before punctuation\n",
    "    s = re.sub(r'\\s([?.!\",](?:\\s|$))', r'\\1', s)\n",
    "    \n",
    "    verbalised_claims_df.loc[i, 'verbalisation_unks_replaced_then_dropped'] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking if every combination of ENGLISH label+alias for (s,p,o) tuples has been covered\n",
    "# NO OUTPUTS = ALL OK\n",
    "\n",
    "claims_list = verbalised_claims_df.claim_id.unique().tolist()\n",
    "for c_id in claims_list:\n",
    "    row = claim_data_df[claim_data_df['claim_id'] == c_id]\n",
    "    matching_claims = row.shape[0]\n",
    "    verbalised_n_rows = verbalised_claims_df[verbalised_claims_df.claim_id == c_id].shape[0]\n",
    "    \n",
    "    verbalised_n_rows_target =\\\n",
    "        ((len(leval(row['entity_alias'].values[0])) if row['entity_alias_lan'].iloc[0] == 'en' else 0) +1) * \\\n",
    "        ((len(leval(row['property_alias'].values[0])) if row['property_alias_lan'].iloc[0] == 'en' else 0) +1) * \\\n",
    "        ((len(leval(row['object_alias'].values[0])) if row['object_alias_lan'].iloc[0] == 'en' else 0) +1)\n",
    "    \n",
    "    try:\n",
    "        assert verbalised_n_rows == verbalised_n_rows_target * matching_claims\n",
    "    except AssertionError:\n",
    "        print('Match Error:',c_id)\n",
    "        print( row.index.values,\n",
    "            verbalised_n_rows, 'out of', verbalised_n_rows_target * matching_claims,\n",
    "            f'{verbalised_n_rows_target * matching_claims - verbalised_n_rows} to go')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbalised_claims_df.to_csv('verbalisation/verbalised_claims_df.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verbalised data CORRECTION and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbalised_claims_df = pd.read_csv('verbalisation/verbalised_claims_df.csv')\n",
    "verbalised_claims_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbalised_claims_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define property aliases to use as main verbalisations\n",
    "special_properties = {\n",
    "    'P1031': 'citation',\n",
    "    'P106': 'profession',\n",
    "    'P1066': 'apprentice of',\n",
    "    'P1196': 'nature of death',\n",
    "    'P1308': 'position holder',\n",
    "    'P131': 'is located in',\n",
    "    'P1346': 'won by',\n",
    "    'P136': {#dict replaces if the key is either in the entity label or the object label\n",
    "        'film': 'film genre', # Manually replace here, as 'genre of' would be good but is not there\n",
    "        '': 'genre' #default\n",
    "    },\n",
    "    'P1435': 'designation', # Manually replace the Overhailes case\n",
    "    'P1441': 'featured in work',\n",
    "    'P1448': 'name',\n",
    "    'P1476': 'titled',\n",
    "    'P1542': 'causes',\n",
    "    'P1559': 'native name',\n",
    "    'P166' : {\n",
    "        'Doctor': 'recognition title',\n",
    "        '': 'award received'\n",
    "    },\n",
    "    'P17': {\n",
    "        'trial': 'host country',\n",
    "        '': 'land'# Manually correct cases where food is introduced (Gups Ponmala)\n",
    "    },\n",
    "    'P186': 'made from',\n",
    "    'P189': 'found in',\n",
    "    'P195': 'art collection',#Manually correct cases due to extensive entity label formatting\n",
    "    'P2017': 'isomeric SMILES', #this is the main label, this is just to remind myself to manually correct them due to extensive label format\n",
    "    'P21': 'gender',\n",
    "    'P233': 'SMILES', #Manually correct cases due to extensive entity label formatting\n",
    "    'P26': 'marry', #Manually correct some cases due to tense\n",
    "    'P279': 'is a type of',\n",
    "    'P2896': 'publication frequency',#Manually correct cases due to missing link between 1 week = weekly expressions.\n",
    "    'P31': 'is a',\n",
    "    'P3373': 'is sibling of',\n",
    "    'P364': 'original language',\n",
    "    'P39' : 'held position',\n",
    "    'P40' : 'has child',\n",
    "    'P451' : 'is partner of',\n",
    "    'P452' : 'sector',#Manually correct line of credit cases\n",
    "    'P485': 'archive location',\n",
    "    'P5021': 'assessment', #Manually correct inversion cases\n",
    "    'P527': 'parts',\n",
    "    'P551': 'resided in',\n",
    "    'P571': 'created',\n",
    "    'P580': 'starting',\n",
    "    'P582': 'ending',\n",
    "    'P607': 'in conflict',\n",
    "    'P674': 'characters', #Manually correct cases here\n",
    "    'P725': 'voice actor', #Manually correct cases here\n",
    "    'P734': 'last name',\n",
    "    'P735': 'first name',\n",
    "    'P780': 'symptoms',\n",
    "    'P793': 'event',\n",
    "    'P802': 'students',\n",
    "    #'P8045' Manually correct this one\n",
    "    'P915': 'filmed at',\n",
    "    'P921': 'about',\n",
    "    'P97': 'hereditary title'\n",
    "}\n",
    "\n",
    "verbalised_claims_df['is_main_verbalisation'] = None\n",
    "verbalised_claims_df['alternative_alias_used'] = None\n",
    "\n",
    "for i, row in verbalised_claims_df.iterrows():\n",
    "    # If a special property (to be replaced by alias), do a custom logic where\n",
    "    # an official property_label is elected instead\n",
    "    if row['property_id'] in special_properties.keys():   \n",
    "        verbalised_claims_df.loc[i, 'alternative_alias_used'] = True\n",
    "        preferred_property_label = special_properties[row['property_id']]\n",
    "        if type(preferred_property_label) == str:\n",
    "            # if the new label is str, just flag as main verbalisation in case the entity/object are main labels\n",
    "            # and the property label is the new official label\n",
    "            if not row['entity_label_is_alias'] and\\\n",
    "                row['property_label'] == preferred_property_label and\\\n",
    "                not row['object_label_is_alias']:\n",
    "                \n",
    "                verbalised_claims_df.loc[i, 'is_main_verbalisation'] = True\n",
    "            else:\n",
    "                verbalised_claims_df.loc[i, 'is_main_verbalisation'] = False\n",
    "        elif type(preferred_property_label) == dict:\n",
    "            # if it's a dict, elect as new official label only if either entity or object labels contain\n",
    "            # the key, and select the property label that is its value\n",
    "            # if one is identified this way, skip the rest\n",
    "            # we start with it not being a main verbalisation until we find a key that matches\n",
    "            verbalised_claims_df.loc[i, 'is_main_verbalisation'] = False\n",
    "            \n",
    "            # check if another row with the same claim id and reference id is \n",
    "            # not already the main verbalisation\n",
    "            if verbalised_claims_df[\n",
    "                (verbalised_claims_df['claim_id'] == row['claim_id']) &\\\n",
    "                (verbalised_claims_df['reference_id'] == row['reference_id'])\n",
    "            ].is_main_verbalisation.sum() > 0:\n",
    "                continue\n",
    "                \n",
    "            for key in preferred_property_label.keys():\n",
    "                if key in row['entity_label'] or key in row['object_label']:\n",
    "                    if not row['entity_label_is_alias'] and\\\n",
    "                        row['property_label'] == preferred_property_label[key] and\\\n",
    "                        not row['object_label_is_alias']:\n",
    "                        #print(row)\n",
    "                        verbalised_claims_df.loc[i, 'is_main_verbalisation'] = True\n",
    "                        if key=='':                            \n",
    "                            verbalised_claims_df.loc[i, 'alternative_alias_used'] = False\n",
    "                        break #stop iterating over keys of preferred_property_label\n",
    "                \n",
    "                \n",
    "    # Else, or if we fail to find an official property_label through the logic above,\n",
    "    # just check if all X_is_alias columns are false\n",
    "    else:        \n",
    "        verbalised_claims_df.loc[i, 'alternative_alias_used'] = False\n",
    "        if not row['entity_label_is_alias'] and\\\n",
    "            not row['property_label_is_alias'] and\\\n",
    "            not row['object_label_is_alias']:\n",
    "\n",
    "            verbalised_claims_df.loc[i, 'is_main_verbalisation'] = True\n",
    "        else:\n",
    "            verbalised_claims_df.loc[i, 'is_main_verbalisation'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert verbalised_claims_df['is_main_verbalisation'].value_counts()[True] == 972\n",
    "#972 is the total of unique ref_claim pairs\n",
    "assert verbalised_claims_df['is_main_verbalisation'].isna().sum() == 0\n",
    "assert verbalised_claims_df['alternative_alias_used'].isna().sum() == 0\n",
    "# no empty cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbalised_claims_df[verbalised_claims_df['is_main_verbalisation'] == True][\n",
    "    verbalised_claims_df[verbalised_claims_df['is_main_verbalisation'] == True].duplicated('claim_id')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total verbalised counts:')\n",
    "print(f'{verbalised_claims_df.claim_id.unique().shape[0]} unique claims')\n",
    "print(f'{verbalised_claims_df.reference_id.unique().shape[0]} unique references')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "n_unk_replacements = verbalised_claims_df[verbalised_claims_df['verbalisation'].apply(lambda x : '<unk>' in x)].shape[0]\n",
    "print(\n",
    "    f\"Unk replacement was needed in {n_unk_replacements} ({100*n_unk_replacements/verbalised_claims_df.shape[0]}%) of verbalisations\"\n",
    ")\n",
    "\n",
    "n_unk_replacements_solved = verbalised_claims_df[verbalised_claims_df['verbalisation_unks_replaced'].apply(lambda x : '<unk>' in x)].shape[0]\n",
    "print(\n",
    "    f\"Unk replacement was NOT solved in {n_unk_replacements_solved} ({100*n_unk_replacements_solved/n_unk_replacements}%) of cases\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of entities and properties involved in the verbalisations\n",
    "\n",
    "fig, ax = plt.subplots(2,2,figsize=(10,8))\n",
    "\n",
    "ax[0][0].set_yscale('log')\n",
    "entity_count = verbalised_claims_df.entity_id.value_counts()\n",
    "ax[0][0].set_title('Boxplot of Entity distribution')\n",
    "sns.boxplot(data = entity_count, ax=ax[0][0])\n",
    "ax[0][1].set_title('KDE of Entity distribution')\n",
    "sns.kdeplot(data = entity_count, ax=ax[0][1])\n",
    "\n",
    "ax[1][0].set_yscale('log')\n",
    "property_count = verbalised_claims_df.property_id.value_counts()\n",
    "ax[1][0].set_title('Boxplot of Property distribution')\n",
    "sns.boxplot(data = property_count, ax=ax[1][0])\n",
    "ax[1][1].set_title('KDE of Property distribution')\n",
    "sns.kdeplot(data = property_count, ax=ax[1][1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Half of both entities and properties show up between 50 and 200/500 times,\n",
    "# the majority being between 1 and 1000. A few outliers are beyond that.\n",
    "\n",
    "# Entities/Properties that are over-represented here are due to having too many aliases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_verbalisation_counts = verbalised_claims_df[['claim_id', 'verbalisation_unks_replaced_then_dropped']].\\\n",
    "    drop_duplicates().claim_id.value_counts()\n",
    "\n",
    "# Distribution of entities and properties involved in the verbalisations\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
    "\n",
    "ax[0].set_yscale('log')\n",
    "ax[0].set_title('Boxplot of Unique verbalisation count\\n distribution per unique claim id')\n",
    "sns.boxplot(data = unique_verbalisation_counts, ax=ax[0])\n",
    "ax[1].set_title('KDE of Unique verbalisation count\\n distribution per unique claim id')\n",
    "sns.kdeplot(data = unique_verbalisation_counts, ax=ax[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# This shows the majority of claims have up to 100 unique verbalisations due to aliases, with some having way more.\n",
    "# The biggest one, for instance, has 33 subject aliases, 22 predicate aliases, and 8 object aliases\n",
    "\n",
    "get_outliers(unique_verbalisation_counts).reset_index().rename({'index':'claim_id','claim_id':'count'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next step is manually correcting some verbalisations before SENTENCE SELECTION\n",
    "#Create a 'corrected_verbalisation' and a 'is_corrected_verbalisation' for manual annotation before sentence selection\n",
    "verbalised_claims_df_main = verbalised_claims_df[verbalised_claims_df['is_main_verbalisation'] == True]\\\n",
    "    .reset_index(drop=True).drop('is_main_verbalisation', axis=1).copy()\n",
    "\n",
    "verbalised_claims_df_main['corrected_verbalisation'] = verbalised_claims_df_main['verbalisation_unks_replaced_then_dropped']\n",
    "verbalised_claims_df_main['is_corrected_verbalisation'] = False\n",
    "\n",
    "verbalised_claims_df_main.to_csv('verbalisation/verbalised_claims_df_main.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of main verbalisations where an alias was used for better verbalisation')\n",
    "print(100*verbalised_claims_df_main['alternative_alias_used'].sum()/verbalised_claims_df_main.shape[0])\n",
    "# Percentage of main verbalisations where an alias was used for better verbalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we manually go over the verbalisations and correct those that need correcting, marking `is_corrected_verbalisation` as true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "verbalised_claims_df_main = pd.read_csv('verbalisation/verbalised_claims_df_main.csv')\n",
    "verbalised_claims_df_main_corrected = pd.read_csv('verbalisation/verbalised_claims_df_main_corrected.csv')\n",
    "\n",
    "# is_corrected_verbalisation has NOT been filled during correction, as this is quicker and less error-prone\n",
    "verbalised_claims_df_main_corrected['is_corrected_verbalisation'] = verbalised_claims_df_main_corrected.apply(\n",
    "    lambda row : row['corrected_verbalisation'] != row['verbalisation_unks_replaced_then_dropped'], axis=1\n",
    ")\n",
    "\n",
    "print('Percentage of main verbalisations where a manual correction was used.')\n",
    "print(100*verbalised_claims_df_main_corrected['is_corrected_verbalisation'].sum()/verbalised_claims_df_main.shape[0])\n",
    "\n",
    "norm_levenshtein_distances = verbalised_claims_df_main_corrected.apply(\n",
    "    lambda row : levenshtein_distance(\n",
    "        row['corrected_verbalisation'],\n",
    "        row['verbalisation_unks_replaced_then_dropped']\n",
    "    )/max(\n",
    "        len(row['corrected_verbalisation']),\n",
    "        len(row['verbalisation_unks_replaced_then_dropped'])\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "norm_levenshtein_distances = norm_levenshtein_distances[norm_levenshtein_distances>0].reset_index(drop=True)\n",
    "sns.boxplot(data=norm_levenshtein_distances, orient='h')\n",
    "print('Distribution of normalised levenshtein distance after corrections.')\n",
    "print(norm_levenshtein_distances.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE P1448 (OFFICIAL NAME), P1476 (TITLE), AND P1889 (DIFFERENT) AS THEY ARE REDUNDANT AND NON-INFORMATIVE\n",
    "#also look at the dataset creation for other properties that were deleted and delete them too\n",
    "\n",
    "BAD_PROPERTIES = [\n",
    "    'P1448', # offical name\n",
    "    'P1476', # title\n",
    "    'P1889',# different\n",
    "    'P31', # - instance of\n",
    "    'P279',# - subclass of\n",
    "    'P373',# - commons category\n",
    "    'P910',# - Topic's main category\n",
    "    'P7561',# - category for the interior of the item\n",
    "    'P5008',# - on focus list of Wikimedia project\n",
    "    'P2670',# -  has parts of the class\n",
    "    'P1740',# -  category for films shot at this location\n",
    "    'P1612',# -  Commons Institution page\n",
    "    'P8989',# -  category for the view of the item\n",
    "    'P2959',# -  permanent duplicated item\n",
    "    'P7867',# -  category for maps\n",
    "    'P935' ,# -  Commons gallery\n",
    "    'P1472',#  -  Commons Creator page\n",
    "    'P8596',# category for the exterior of the item\n",
    "    'P5105',# Deutsche Bahn station category\n",
    "    'P8933',# category for the view from the item\n",
    "    'P642',# of\n",
    "    'P3876',# category for alumni of educational institution\n",
    "    'P1791',# category of people buried here\n",
    "    'P7084',# related category\n",
    "    'P1465',# category for people who died here\n",
    "    'P1687',# Wikidata property\n",
    "    'P6104',# maintained by WikiProject\n",
    "    'P4195',# category for employees of the organization\n",
    "    'P1792',# category of associated people\n",
    "    'P5869',# model item\n",
    "    'P1659',# see also\n",
    "    'P1464',# category for people born here\n",
    "    'P2354',# has list\n",
    "    'P1424',# topic's main template\n",
    "    'P7782',# category for ship name\n",
    "    'P179',# part of the series\n",
    "    'P7888',# merged into\n",
    "    'P6365',# member category\n",
    "    'P8464',# content partnership category\n",
    "    'P360',# is a list of\n",
    "    'P805',# statement is subject of\n",
    "    'P8703',# entry in abbreviations table\n",
    "    'P1456',# list of monuments\n",
    "    'P1012',# including\n",
    "    'P1151',# topic's main Wikimedia portal\n",
    "    'P2490',# page at OSTIS Belarus Wiki\n",
    "    'P593',# HomoloGene ID\n",
    "    'P8744',# economy of topic\n",
    "    'P2614',# World Heritage criteria\n",
    "    'P2184',# history of topic\n",
    "    'P9241',# demographics of topic\n",
    "    'P487',#Unicode character\n",
    "    'P1754',#category related to list\n",
    "    'P2559',#Wikidata usage instructions\n",
    "    'P2517',#category for recipients of this award\n",
    "    'P971',#category combines topics\n",
    "    'P6112',# category for members of a team\n",
    "    'P4224',#category contains\n",
    "    'P301',#category's main topic\n",
    "    'P1753',#list related to category\n",
    "    'P1423',#template has topic\n",
    "    'P1204',#Wikimedia portal's main topic\n",
    "    'P3921',#Wikidata SPARQL query equivalent\n",
    "    'P1963',#properties for this type\n",
    "    'P5125',#Wikimedia outline\n",
    "    'P3176',#uses property\n",
    "    'P8952',#inappropriate property for this type\n",
    "    'P2306',#property\n",
    "    'P5193',#Wikidata property example for forms\n",
    "    'P5977',#Wikidata property example for senses\n",
    "    'P1748',#NCI Thesaurus ID\n",
    "    'P1692',#ICD-9-CM\n",
    "    'P248',#stated in\n",
    "]\n",
    "\n",
    "verbalised_claims_df_main_corrected_badpropdrop = verbalised_claims_df_main_corrected[\n",
    "    ~verbalised_claims_df_main_corrected['property_id'].isin(BAD_PROPERTIES)\n",
    "]\n",
    "print('Percentage [Number] of claims dropped due to bad properties')\n",
    "print(\n",
    "    f'{100 - 100*verbalised_claims_df_main_corrected_badpropdrop.shape[0]/verbalised_claims_df_main_corrected.shape[0]}',\n",
    "    f'[{verbalised_claims_df_main_corrected.shape[0] - verbalised_claims_df_main_corrected_badpropdrop.shape[0]}]'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Correct some final_urls in the reference_text_df dataframe before joining\n",
    "import re\n",
    "\n",
    "# Replace by archived page if page was behing paywall when parsed\n",
    "reference_text_df.loc[reference_text_df.html.map(lambda x : '://archive.ph/' in x), 'final_url'] =\\\n",
    "    reference_text_df.loc[reference_text_df.html.map(lambda x : '://archive.ph/' in x)]\\\n",
    "        .html.map(lambda x : re.findall(r'http(?:s){0,1}://archive.ph/(?:[a-zA-Z0-9]*)', x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get URLs from the references df\n",
    "verbalised_claims_df_main_corrected_badpropdrop_url = \\\n",
    "    pd.merge(\n",
    "    verbalised_claims_df_main_corrected_badpropdrop,\n",
    "    reference_text_df[['reference_id', 'final_url']],\n",
    "    on='reference_id'\n",
    ")\n",
    "verbalised_claims_df_main_corrected_badpropdrop_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates of reference and verbalisation, as duplicates arise from qualifier dependancy\n",
    "verbalised_claims_df_main_corrected_badpropdrop_url_duplidrop = \\\n",
    "    verbalised_claims_df_main_corrected_badpropdrop_url.drop_duplicates(\n",
    "    ['corrected_verbalisation','final_url'], keep='first'\n",
    ")\n",
    "\n",
    "print('Percentage [Number] of claims dropped due to duplicated verbalisation and url pair')\n",
    "print(\n",
    "    f'{100 - 100*verbalised_claims_df_main_corrected_badpropdrop_url_duplidrop.shape[0]/verbalised_claims_df_main_corrected_badpropdrop_url.shape[0]}',\n",
    "    f'[{verbalised_claims_df_main_corrected_badpropdrop_url.shape[0] - verbalised_claims_df_main_corrected_badpropdrop_url_duplidrop.shape[0]}]'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the three cases in archinform.net written in German\n",
    "verbalised_claims_df_main_corrected_badpropdrop_url_duplidrop = verbalised_claims_df_main_corrected_badpropdrop_url_duplidrop[\n",
    "    ~verbalised_claims_df_main_corrected_badpropdrop_url_duplidrop['final_url'].map(\n",
    "        lambda x : 'www.archinform.net' in x and any([(y in x) for y in ['19632','11996','45859']])\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbalised_claims_df_main_corrected_badpropdrop_url_duplidrop.reset_index(drop=True, inplace=True)\n",
    "verbalised_claims_df_main_corrected_badpropdrop_url_duplidrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following claims were selected randomly such that\n",
    "# each reference_id had only one claim selected.\n",
    "# However, we did not keep the seed which generated it.\n",
    "randomly_selected_rows = [\n",
    "    0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 18, 22, 23, 27, 29, 32, 36, 39, 43, 44, 49,\n",
    "    51, 58, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 80, 81, 82, 83, 84, 85, 86,\n",
    "    87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107,\n",
    "    108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 120, 122, 123, 125, 126, 128, 130, 131,\n",
    "    132, 133, 135, 136, 140, 141, 142, 147, 151, 152, 153, 154, 155, 158, 159, 160, 161, 163,\n",
    "    164, 166, 168, 169, 170, 171, 174, 175, 176, 177, 178, 185, 186, 188, 190, 193, 194, 197,\n",
    "    198, 200, 202, 204, 206, 207, 211, 212, 214, 217, 219, 220, 222, 224, 229, 230, 231, 233,\n",
    "    235, 239, 240, 241, 243, 244, 246, 247, 250, 251, 253, 254, 255, 256, 257, 258, 259, 260,\n",
    "    261, 262, 263, 264, 265, 266, 268, 269, 270, 271, 272, 274, 276, 277, 280, 281, 283, 285,\n",
    "    286, 288, 290, 292, 294, 299, 300, 301, 302, 303, 312, 319, 323, 324, 325, 326, 327, 328,\n",
    "    331, 332, 335, 336, 338, 340, 342, 344, 345, 347, 350, 352, 353, 354, 355, 356, 357, 358,\n",
    "    359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 371, 372, 373, 375, 376, 377, 378,\n",
    "    379, 380, 382, 389, 391, 399, 407, 409, 414, 415, 416, 418, 419, 420, 422, 423, 424, 425,\n",
    "    426, 427, 428, 429, 432, 439, 442, 443, 445, 446, 448, 454, 456, 458, 461, 464, 465, 467,\n",
    "    468, 470, 472, 475, 476, 478, 481, 483, 484, 486, 488, 491, 492, 496, 499, 503, 508, 509,\n",
    "    516, 519, 524, 526, 528, 536, 537, 539, 540, 542, 544, 545, 546, 547, 548, 551, 552, 557,\n",
    "    561, 570, 571, 573, 576, 578, 579, 581, 583, 584, 586, 588, 589, 590, 592, 593, 594, 595,\n",
    "    596, 597, 598, 599, 600, 601, 604, 606, 607, 613, 614, 615, 616, 617, 619, 620, 621, 629,\n",
    "    635, 642, 644, 645, 646, 650, 656, 665, 666, 673, 681, 687, 694, 696, 707, 713, 717, 721,\n",
    "    729, 730, 731, 732, 736, 737, 738, 739, 740, 741, 743, 744, 745, 747, 748, 749, 750, 751,\n",
    "    752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 764, 766, 768, 769, 772, 773, 774,\n",
    "    775, 776, 777, 779, 780, 781, 782, 784, 785, 786, 787, 790, 791, 792, 793, 795\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbalised_claims_df_final = verbalised_claims_df_main_corrected_badpropdrop_url_duplidrop\n",
    "verbalised_claims_df_final = verbalised_claims_df_final.loc[randomly_selected_rows]\n",
    "verbalised_claims_df_final = verbalised_claims_df_final.reset_index(drop=True)\n",
    "verbalised_claims_df_final.to_csv('verbalisation/verbalised_claims_df_final.csv', index=None)\n",
    "verbalised_claims_df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sentence Selection\n",
    "\n",
    "Due to the HUGE amounts of verbalisations per unique claim, we will now only look at the **MAIN verbalisations**, defined previously (see last section) as those having main labels only or entity/object main labels and a preferred predicate label.\n",
    "\n",
    "As next steps, we **COULD**:\n",
    "1. Select main label verbalisations + 10 others at random from the whole set\n",
    "2. Select main label verbalisations + 10 others at random given that they (subj and obj) *appear on the document*\n",
    "3. Select main label verbalisations + 10 others based on some scoring function\n",
    "\n",
    "Here, we **WILL**:\n",
    "1. Combine the 1302 (formerly 1570 before cutting some entries down in the last few cells of last section) verbalised claims with their respective reference contents (as sentences).\n",
    "2. Then, we will calculate a relevance score for each sentence, and keep only the (up-to) 5 best positive scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbalised_claims_df_final = pd.read_csv('verbalisation/verbalised_claims_df_final.csv')\n",
    "verbalised_claims_df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove redundant columns and unnecessary columns for the merging with reference contents data\n",
    "verbalised_claims_df_final = verbalised_claims_df_final[[\n",
    "    'reference_id', 'claim_id', 'corrected_verbalisation'\n",
    "]]\n",
    "verbalised_claims_df_final = verbalised_claims_df_final.rename(\n",
    "    {'corrected_verbalisation': 'final_verbalisation'},\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Take only the data relevant for joining with the verbalisation data and calculating sentence relevance scores\n",
    "\n",
    "# The sampling weights CAN ONLY BE USED to average-out any score or quantifiable property that is assigned to EACH REFERENCE,\n",
    "# such as: percentage of claims actually supported by the reference out of all claims linked to it according\n",
    "# to pipeline results\n",
    "\n",
    "reference_text_df_for_sentence_selection_join = reference_text_df[[\n",
    "    'reference_id', 'sampling_weight', 'final_url', 'netloc_agg', 'nlp_sentences', 'nlp_sentences_slide_2'\n",
    "]]\n",
    "\n",
    "\n",
    "sentence_relevance_df = pd.merge(\n",
    "    verbalised_claims_df_final,\n",
    "    reference_text_df_for_sentence_selection_join,\n",
    "    how='left',\n",
    "    on='reference_id'\n",
    ")\n",
    "\n",
    "sentence_relevance_df['nlp_sentences'] = sentence_relevance_df['nlp_sentences'].apply(leval)\n",
    "sentence_relevance_df['nlp_sentences_slide_2'] = sentence_relevance_df['nlp_sentences_slide_2'].apply(leval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_retrieval import sentence_retrieval_module\n",
    "\n",
    "# If updating the module\n",
    "#from importlib import reload\n",
    "#reload(sentence_retrieval_module)\n",
    "\n",
    "sr_module = sentence_retrieval_module.SentenceRetrievalModule(max_len=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SHIFTING MAIN LABELS OF PROBLEMATIC LABELS FOR VERBALISATION**\n",
    "\n",
    "Some properties, such as \"child\" and \"date of inception\" are problematic for reasons we discussed in a previous paper. So, for better performance, we might shift the main label flags over to more appropriate aliases, like shifting from \"child\" to \"has child\", and \"date of inception\" to \"data of creation\".\n",
    "\n",
    "To change:\n",
    "    - P?: \"child\" to \"has child\"\n",
    "    - P?: \"inception\" to \"?\"\n",
    "    - ??\n",
    "    \n",
    "**DONE**: Check section 3, where this and other changes were made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "sentence_relevance_df['nlp_sentences_scores'] = None\n",
    "sentence_relevance_df['nlp_sentences_slide_2_scores'] = None\n",
    "\n",
    "def chunks(l, n):\n",
    "    n = max(1, n)\n",
    "    return [l[i:i+n] for i in range(0, len(l), n)]\n",
    "\n",
    "all_outputs = []\n",
    "for i, row in tqdm(sentence_relevance_df.iterrows(), total=sentence_relevance_df.shape[0]):\n",
    "    \n",
    "    outputs = []\n",
    "    for batch in chunks(row['nlp_sentences'], BATCH_SIZE):\n",
    "        batch_outputs = sr_module.score_sentence_pairs(\n",
    "            [(row['final_verbalisation'], sentence) for sentence in batch]\n",
    "        )\n",
    "        outputs += batch_outputs\n",
    "    all_outputs.append(outputs)\n",
    "    \n",
    "all_outputs = pd.Series(all_outputs)\n",
    "sentence_relevance_df['nlp_sentences_scores'] = all_outputs\n",
    "\n",
    "assert all(sentence_relevance_df.apply(\n",
    "    lambda x : len(x['nlp_sentences']) == len(x['nlp_sentences_scores']),\n",
    "    axis=1\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_outputs = []\n",
    "for i, row in tqdm(sentence_relevance_df.iterrows(), total=sentence_relevance_df.shape[0]):\n",
    "    \n",
    "    outputs = []\n",
    "    for batch in chunks(row['nlp_sentences_slide_2'], BATCH_SIZE):\n",
    "        batch_outputs = sr_module.score_sentence_pairs(\n",
    "            [(row['final_verbalisation'], sentence) for sentence in batch]\n",
    "        )\n",
    "        outputs += batch_outputs\n",
    "    all_outputs.append(outputs)\n",
    "    \n",
    "all_outputs = pd.Series(all_outputs)    \n",
    "sentence_relevance_df['nlp_sentences_slide_2_scores'] = all_outputs\n",
    "    \n",
    "\n",
    "assert all(sentence_relevance_df.apply(\n",
    "    lambda x : len(x['nlp_sentences_slide_2']) == len(x['nlp_sentences_slide_2_scores']),\n",
    "    axis=1\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOP_SENTENCES = 5\n",
    "SCORE_THRESHOLD = 0\n",
    "\n",
    "nlp_sentences_TOP_N = []\n",
    "nlp_sentences_slide_2_TOP_N = []\n",
    "nlp_sentences_all_TOP_N = []\n",
    "\n",
    "for i, row in tqdm(sentence_relevance_df.iterrows(), total=sentence_relevance_df.shape[0]):\n",
    "\n",
    "    nlp_sentences_with_scores = [{\n",
    "        'sentence': t[0],\n",
    "        'score': t[1],\n",
    "        'sentence_id': str(j)\n",
    "    } for j, t in enumerate(zip(row['nlp_sentences'], row['nlp_sentences_scores']))]\n",
    "\n",
    "    nlp_sentences_with_scores = sorted(nlp_sentences_with_scores, key = lambda x : x['score'], reverse = True)\n",
    "    nlp_sentences_TOP_N.append(nlp_sentences_with_scores[:N_TOP_SENTENCES])\n",
    "    \n",
    "    nlp_sentences_slide_2_with_scores = [{\n",
    "        'sentence': t[0],\n",
    "        'score': t[1],\n",
    "        'sentence_id': str(j)+';'+str(j+1)\n",
    "    } for j, t in enumerate(zip(row['nlp_sentences_slide_2'], row['nlp_sentences_slide_2_scores']))]\n",
    "\n",
    "    nlp_sentences_slide_2_with_scores = sorted(nlp_sentences_slide_2_with_scores, key = lambda x : x['score'], reverse = True)\n",
    "    nlp_sentences_slide_2_TOP_N.append(nlp_sentences_slide_2_with_scores[:N_TOP_SENTENCES])\n",
    "    \n",
    "\n",
    "    nlp_sentences_all_with_scores = nlp_sentences_with_scores + nlp_sentences_slide_2_with_scores\n",
    "    nlp_sentences_all_with_scores = sorted(nlp_sentences_all_with_scores, key = lambda x : x['score'], reverse = True)\n",
    "    \n",
    "    #We might no want to allow overlaps, so we do the following:\n",
    "    #For each evidence in descending order of score, we delete from the 'all' list\n",
    "    #all overlapping evidence scored lower than it\n",
    "    nlp_sentences_all_with_scores_filtered_for_overlap = []\n",
    "    for evidence in nlp_sentences_all_with_scores:\n",
    "        if ';' in evidence['sentence_id']:\n",
    "            [start_id, end_id] = evidence['sentence_id'].split(';')\n",
    "            if not any(\n",
    "                [start_id in e['sentence_id'].split(';') for e in nlp_sentences_all_with_scores_filtered_for_overlap]\n",
    "            ):\n",
    "                if not any(\n",
    "                    [end_id in e['sentence_id'].split(';') for e in nlp_sentences_all_with_scores_filtered_for_overlap]\n",
    "                ):\n",
    "                    nlp_sentences_all_with_scores_filtered_for_overlap.append(evidence)\n",
    "        else:\n",
    "            if not any(\n",
    "                [evidence['sentence_id'] in e['sentence_id'].split(';') for e in nlp_sentences_all_with_scores_filtered_for_overlap]\n",
    "            ):\n",
    "                nlp_sentences_all_with_scores_filtered_for_overlap.append(evidence)\n",
    "    \n",
    "    \n",
    "    assert len(nlp_sentences_all_with_scores_filtered_for_overlap) >= 5    \n",
    "    nlp_sentences_all_TOP_N.append(nlp_sentences_all_with_scores_filtered_for_overlap[:N_TOP_SENTENCES])\n",
    "    \n",
    "sentence_relevance_df['nlp_sentences_TOP_N'] = pd.Series(nlp_sentences_TOP_N)\n",
    "sentence_relevance_df['nlp_sentences_slide_2_TOP_N'] = pd.Series(nlp_sentences_slide_2_TOP_N)\n",
    "sentence_relevance_df['nlp_sentences_all_TOP_N'] = pd.Series(nlp_sentences_all_TOP_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_relevance_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_relevance_df.iloc[1].nlp_sentences_all_TOP_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_relevance_df.to_json(\n",
    "    'sentence_retrieval/sentence_relevance_df.json', orient='records', indent=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_relevance_df = pd.read_json('sentence_retrieval/sentence_relevance_df.json')\n",
    "sentence_relevance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual analysis of selected sentences\n",
    "As will quantitative metrics later, for now let's try just looking at some examples by hand.\n",
    "\n",
    "We will, for all netloc_aggs, choose a random sample and check if selected evidence make sense given claim.\n",
    "\n",
    "**Notes**:\n",
    "- Threshold variation might be something we can use the crowdsourcing to better define.\n",
    "- *Sometimes* even sentences with negative scores, when ALL sentences are very poorly scored, might be useful. We could show that to users and say \"our models ranked this sentence as low relevance\" (or maybe with a color scheme or 1/5 starts symbol), but show anyway, and ask users how helpful they truly were.\n",
    "    - Basically, ask the crowd if our scores of relevancy/support make sense and correlate to their own judgements.\n",
    "- Many cases of failure involve information that is written in tabular form and depend on humans infering information/context from the page's layout.\n",
    "    - Example is with \"The chemical formula for radium ra-223 dichloride is Cl2Ra.\" and the url https://go.drugbank.com/drugs/DB08913. No sentence extracted links the compound's name to the formula.\n",
    "- Many cases of failure involve information that requires multi-hop reasoning, where the information is broken in multiple places in the page.\n",
    "    - Such an example is with \"Naravoor Lps is located in India.\" and the url https://schools.org.in/Kannur/32020700608/, where one sentence shows the school is in Kannur, and another that Kannur is in India.\n",
    "- Some cases of failure involve common sense or background knowledge, which is needed to infer the information from the page.\n",
    "    - For example, the \"Salafi Eng.Medium Lps is a lower primary school.\" sentence and the url https://schools.org.in/malappuram/32051000519/. There, it is stated the school has grades from 1 to 4, which is what a lower primary school consists of, but that information is not found in the page and depends on the reader's knowledge.\n",
    "    - Or \"William Sleator died on 03/08/2011.\" and http://archive.today/vSGie., which only states he \"died on Wednesday\".\n",
    "    - Another is that the title of something is something, or that it is written in English, etc. Things we can clearly check, but are not written anywhere in the page.\n",
    "    - Or \"HealthRight International is based in the United States of America.\" and https://www.charitynavigator.org/ein/133791391, stating an address in NY, but we have to infer having a contact address in NY means it is based on NY, which is in the US.\n",
    "- Some are a mix of layout and common sense\n",
    "    - Like \"Julius (painting) has a thickness of 1.5 centimetres.\" and the url https://www.rct.uk/collection/402419/julius, which has a sentence \"Julius Signed and dated 1846. Oil on canvas | 53.1 x 65.4 x 1.5 cm\" receiving low score because the fact that the LAST metric is the thickness is infered through common sense.\n",
    "- Some are because the information is on a page REACHED THROUGH the linked page, not on itself.\n",
    "- Some are due to highly domain-specific jargon and acronyms\n",
    "    - \"Capsazepine interacts physically with Transient receptor potential cation channel, subfamily M, member 8.\" links to https://www.guidetopharmacology.org/GRAC/LigandDisplayForward?ligandId=2461, where the object is called TRPM8\n",
    "- Some are due to reduntant info that is not going to be on the source anyway, making the source rather unnecessary\n",
    "    - Like \"Supply Act (No. 2) 1965-66 has the title Supply Act (No. 2) 1965-66.\"\n",
    "- Some are subjective:\n",
    "    - \"Agnes Alinschaw's trial was a significant event.\"\n",
    "- Some depend on qualifiers:\n",
    "    - \"2001: A Space Odyssey was assessed using the Bechdel test.\" depends on the qualifier \"assessment outcome: fails\".\n",
    "\n",
    "\n",
    "To alter:\n",
    "- **Remember** to alter the URLs leading to paywall-protected pages by using their https://archive.ph/ version.\n",
    "    - You can build a list by checking if 'archive.ph' is in the html.\n",
    "- A handful of websites are not in English, we can get a list from netloc archinform.net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## To see the specific sentence/score breakdown for a specific verbalisation\n",
    "#row = sentence_relevance_df[sentence_relevance_df['final_verbalisation'] == 'Pokinatcha is a genre of pop rock.']\n",
    "#list(zip(row.nlp_sentences_slide_2.tolist()[0], row.nlp_sentences_slide_2_scores.tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for netloc in sentence_relevance_df.netloc_agg.unique().tolist():\n",
    "    print('-'*10)\n",
    "    print(netloc,'\\n')\n",
    "    sampled_rows = sentence_relevance_df[sentence_relevance_df['netloc_agg'] == netloc].sample(3, random_state=42)\n",
    "    for i, row in sampled_rows.iterrows():\n",
    "        print('->', row['final_verbalisation'])\n",
    "        print('->>', row['final_url'])\n",
    "        for i, sentence in enumerate(row['nlp_sentences_all_TOP_N']):\n",
    "            print(f\"\\t{i+1}. {sentence['sentence']} ({sentence['score']})\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Textual Entailment\n",
    "\n",
    "Now, for each claim, we take the Top sentences alongside their scores and pass through the textual entalmente module, outputing the following:\n",
    "- Scores (three scores, each for a label) of each of the N sentences\n",
    "- Labels of each of the N sentences based on the scores\n",
    "- Final weighted scores of the sentences (used on the weighted sum method)\n",
    "- Final label of the claim based on two methods, weighted sum and malon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textual_entailment import textual_entailment_module\n",
    "\n",
    "from importlib import reload\n",
    "reload(textual_entailment_module)\n",
    "\n",
    "te_module = textual_entailment_module.TextualEntailmentModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textual_entailment_df = sentence_relevance_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORE_THRESHOLD = 0\n",
    "\n",
    "keys = ['TOP_N', 'slide_2_TOP_N', 'all_TOP_N']\n",
    "te_columns = {}\n",
    "\n",
    "for key in keys:\n",
    "    te_columns[f'evidence_TE_prob_{key}'] = []\n",
    "    te_columns[f'evidence_TE_prob_weighted_{key}'] = []\n",
    "    te_columns[f'evidence_TE_labels_{key}'] = []\n",
    "    te_columns[f'claim_TE_prob_weighted_sum_{key}'] = []\n",
    "    te_columns[f'claim_TE_label_weighted_sum_{key}'] = []\n",
    "    te_columns[f'claim_TE_label_malon_{key}'] = []\n",
    "\n",
    "\n",
    "for i, row in tqdm(textual_entailment_df.iterrows(), total=textual_entailment_df.shape[0]):\n",
    "    try:\n",
    "        claim = row['final_verbalisation']\n",
    "\n",
    "        result_sets = {key : {'evidence': row[f'nlp_sentences_{key}']} for key in keys}\n",
    "\n",
    "        for key, rs in result_sets.items():\n",
    "\n",
    "            evidence_size = len([e for e in rs['evidence']])\n",
    "           \n",
    "            rs['evidence_TE_prob'] = te_module.get_batch_scores(\n",
    "                claims = [claim for _ in range(evidence_size)],\n",
    "                evidence = [e['sentence'] for e in rs['evidence']]\n",
    "            )   \n",
    "            \n",
    "            rs['evidence_TE_labels'] = [te_module.get_label_from_scores(s) for s in rs['evidence_TE_prob']]\n",
    "                \n",
    "            rs['evidence_TE_prob_weighted'] = [\n",
    "                probs*ev['score'] for probs, ev in zip(rs['evidence_TE_prob'], rs['evidence'])\\\n",
    "                if ev['score'] > SCORE_THRESHOLD\n",
    "            ]\n",
    "            \n",
    "            rs['claim_TE_prob_weighted_sum'] = \\\n",
    "                np.sum(rs['evidence_TE_prob_weighted'], axis=0)\\\n",
    "                if rs['evidence_TE_prob_weighted'] else [0,0,0]\n",
    "            \n",
    "            rs['claim_TE_label_weighted_sum'] = \\\n",
    "                te_module.get_label_from_scores(rs['claim_TE_prob_weighted_sum'])\\\n",
    "                if rs['evidence_TE_prob_weighted'] else 'NOT ENOUGH INFO'  \n",
    "            \n",
    "\n",
    "            rs['claim_TE_label_malon'] = te_module.get_label_malon(\n",
    "                probs for probs, ev in zip(rs['evidence_TE_prob'], rs['evidence'])\\\n",
    "                if ev['score'] > SCORE_THRESHOLD\n",
    "            )\n",
    "\n",
    "            te_columns[f'evidence_TE_prob_{key}'].append(rs['evidence_TE_prob'])\n",
    "            te_columns[f'evidence_TE_prob_weighted_{key}'].append(rs['evidence_TE_prob_weighted'])\n",
    "            te_columns[f'evidence_TE_labels_{key}'].append(rs['evidence_TE_labels'])\n",
    "            te_columns[f'claim_TE_prob_weighted_sum_{key}'].append(rs['claim_TE_prob_weighted_sum'])\n",
    "            te_columns[f'claim_TE_label_weighted_sum_{key}'].append(rs['claim_TE_label_weighted_sum'])\n",
    "            te_columns[f'claim_TE_label_malon_{key}'].append(rs['claim_TE_label_malon'])\n",
    "            \n",
    "            #print(rs)\n",
    "            #break\n",
    "    \n",
    "    except Exception:\n",
    "        print(row)\n",
    "        print(result_sets)\n",
    "\n",
    "        raise\n",
    "    \n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in keys:\n",
    "    textual_entailment_df[f'evidence_TE_prob_{key}'] = pd.Series(te_columns[f'evidence_TE_prob_{key}'])\n",
    "    textual_entailment_df[f'evidence_TE_prob_weighted_{key}'] = pd.Series(te_columns[f'evidence_TE_prob_weighted_{key}'])\n",
    "    textual_entailment_df[f'evidence_TE_labels_{key}'] = pd.Series(te_columns[f'evidence_TE_labels_{key}'])\n",
    "    textual_entailment_df[f'claim_TE_prob_weighted_sum_{key}'] = pd.Series(te_columns[f'claim_TE_prob_weighted_sum_{key}'])\n",
    "    textual_entailment_df[f'claim_TE_label_weighted_sum_{key}'] = pd.Series(te_columns[f'claim_TE_label_weighted_sum_{key}'])\n",
    "    textual_entailment_df[f'claim_TE_label_malon_{key}'] = pd.Series(te_columns[f'claim_TE_label_malon_{key}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textual_entailment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textual_entailment_df.to_json('textual_entailment/textual_entailment_df.json', orient=\"records\", indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textual_entailment_df = pd.read_json('textual_entailment/textual_entailment_df.json')\n",
    "SCORE_THRESHOLD = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textual_entailment_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textual_entailment_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3,figsize=(15,5))\n",
    "\n",
    "sns.histplot(\n",
    "    textual_entailment_df.nlp_sentences_TOP_N.apply(lambda x : len([xx for xx in x if xx['score'] > SCORE_THRESHOLD])),\n",
    "    ax=ax[0], stat='probability', discrete=True\n",
    ")\n",
    "sns.histplot(\n",
    "    textual_entailment_df.nlp_sentences_slide_2_TOP_N.apply(lambda x : len([xx for xx in x if xx['score'] > SCORE_THRESHOLD])),\n",
    "    ax=ax[1], stat='probability', discrete=True\n",
    ")\n",
    "sns.histplot(\n",
    "    textual_entailment_df.nlp_sentences_all_TOP_N.apply(lambda x : len([xx for xx in x if xx['score'] > SCORE_THRESHOLD])),\n",
    "    ax=ax[2], stat='probability', discrete=True\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "\n",
    "df = textual_entailment_df[['netloc_agg', 'nlp_sentences_all_TOP_N']].copy()\n",
    "\n",
    "df['nlp_sentences_all_TOP_N_score_mean'] = df.nlp_sentences_all_TOP_N.apply(\n",
    "    lambda x : np.mean([xx['score'] for xx in x] if len(x) > 0 else 0)\n",
    ")\n",
    "df['nlp_sentences_all_TOP_N_score_max'] = df.nlp_sentences_all_TOP_N.apply(\n",
    "    lambda x : np.max([xx['score'] for xx in x] if len(x) > 0 else 0)\n",
    ")\n",
    "df['nlp_sentences_all_TOP_N_score_min'] = df.nlp_sentences_all_TOP_N.apply(\n",
    "    lambda x : np.min([xx['score'] for xx in x] if len(x) > 0 else 0)\n",
    ")\n",
    "df = df.groupby('netloc_agg').mean().reset_index()\n",
    "df = df.sort_values('nlp_sentences_all_TOP_N_score_mean', ascending=False).reset_index(drop=True)\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    plt.hlines(\n",
    "        y=i,\n",
    "        xmin=row['nlp_sentences_all_TOP_N_score_min'],\n",
    "        xmax=row['nlp_sentences_all_TOP_N_score_max'],\n",
    "        color='black'\n",
    "    )\n",
    "\n",
    "sns.swarmplot(\n",
    "    data=df,\n",
    "    x='nlp_sentences_all_TOP_N_score_mean',\n",
    "    y='netloc_agg',\n",
    "    order=df.netloc_agg,\n",
    "    ax=ax,\n",
    "    linewidth=1\n",
    ")\n",
    "sns.swarmplot(\n",
    "    data=df,\n",
    "    x='nlp_sentences_all_TOP_N_score_max',\n",
    "    y='netloc_agg',\n",
    "    order=df.netloc_agg,\n",
    "    ax=ax,\n",
    "    linewidth=1\n",
    ")\n",
    "sns.swarmplot(\n",
    "    data=df,\n",
    "    x='nlp_sentences_all_TOP_N_score_min',\n",
    "    y='netloc_agg',\n",
    "    order=df.netloc_agg,\n",
    "    ax=ax,\n",
    "    linewidth=1\n",
    ")\n",
    "\n",
    "plt.axvline(x=1.00, c='black', ls='--', alpha=0.25)\n",
    "plt.axvline(x=0.75, c='black', ls='--', alpha=0.25)\n",
    "plt.axvline(x=0.50, c='black', ls='--', alpha=0.25)\n",
    "plt.axvline(x=0.25, c='black', ls='--', alpha=0.25)\n",
    "plt.axvline(x=0.00, c='black', ls='--', alpha=0.25)\n",
    "plt.axvline(x=-0.75, c='black', ls='--', alpha=0.25)\n",
    "plt.axvline(x=-0.50, c='black', ls='--', alpha=0.25)\n",
    "plt.axvline(x=-0.25, c='black', ls='--', alpha=0.25)\n",
    "\n",
    "plt.ylabel('Netlocs')\n",
    "plt.xlabel('Means of evidence scores above threshold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# generate a version of this without bad netlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "\n",
    "df = textual_entailment_df[['netloc_agg', 'nlp_sentences_all_TOP_N']].copy()\n",
    "\n",
    "df['nlp_sentences_all_TOP_N_score_mean'] = df.nlp_sentences_all_TOP_N\\\n",
    "    .apply(lambda x : [xx for xx in x if xx['score'] > SCORE_THRESHOLD])\\\n",
    "    .apply(\n",
    "        lambda x : np.mean([xx['score'] for xx in x] if len(x) > 0 else 0)\n",
    "    )\n",
    "\n",
    "df['nlp_sentences_all_TOP_N_score_max'] = df.nlp_sentences_all_TOP_N\\\n",
    "    .apply(lambda x : [xx for xx in x if xx['score'] > SCORE_THRESHOLD])\\\n",
    "    .apply(\n",
    "        lambda x : np.max([xx['score'] for xx in x] if len(x) > 0 else 0)\n",
    "    )\n",
    "\n",
    "df['nlp_sentences_all_TOP_N_score_min'] = df.nlp_sentences_all_TOP_N\\\n",
    "    .apply(lambda x : [xx for xx in x if xx['score'] > SCORE_THRESHOLD])\\\n",
    "    .apply(\n",
    "        lambda x : np.min([xx['score'] for xx in x] if len(x) > 0 else 0)\n",
    "    )\n",
    "\n",
    "df = df.groupby('netloc_agg').mean().reset_index()\n",
    "df = df.sort_values('nlp_sentences_all_TOP_N_score_mean', ascending=False).reset_index(drop=True)\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    plt.hlines(\n",
    "        y=i,\n",
    "        xmin=row['nlp_sentences_all_TOP_N_score_min'],\n",
    "        xmax=row['nlp_sentences_all_TOP_N_score_max'],\n",
    "        color='black'\n",
    "    )\n",
    "\n",
    "sns.swarmplot(\n",
    "    data=df,\n",
    "    x='nlp_sentences_all_TOP_N_score_mean',\n",
    "    y='netloc_agg',\n",
    "    order=df.netloc_agg,\n",
    "    ax=ax,\n",
    "    linewidth=1\n",
    ")\n",
    "sns.swarmplot(\n",
    "    data=df,\n",
    "    x='nlp_sentences_all_TOP_N_score_max',\n",
    "    y='netloc_agg',\n",
    "    order=df.netloc_agg,\n",
    "    ax=ax,\n",
    "    linewidth=1\n",
    ")\n",
    "sns.swarmplot(\n",
    "    data=df,\n",
    "    x='nlp_sentences_all_TOP_N_score_min',\n",
    "    y='netloc_agg',\n",
    "    order=df.netloc_agg,\n",
    "    ax=ax,\n",
    "    linewidth=1\n",
    ")\n",
    "\n",
    "plt.axvline(x=0.75, c='black', ls='--', alpha=0.25)\n",
    "plt.axvline(x=0.50, c='black', ls='--', alpha=0.25)\n",
    "plt.axvline(x=0.25, c='black', ls='--', alpha=0.25)\n",
    "\n",
    "plt.ylabel('Netlocs')\n",
    "plt.xlabel('Means of evidence scores above threshold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "\n",
    "df = textual_entailment_df[['netloc_agg', 'nlp_sentences_all_TOP_N']].copy()\n",
    "\n",
    "df['nlp_sentences_all_TOP_N_nonzero'] = df.nlp_sentences_all_TOP_N\\\n",
    "    .apply(lambda x : len([xx for xx in x if xx['score'] > SCORE_THRESHOLD]) != 0)\n",
    "df['count'] = 1\n",
    "df = df.groupby('netloc_agg').sum().reset_index()\n",
    "df['nlp_sentences_all_TOP_N_nonzero_perc'] = 100 * df['nlp_sentences_all_TOP_N_nonzero'] / df['count']\n",
    "sns.barplot(\n",
    "    data=df,\n",
    "    x='nlp_sentences_all_TOP_N_nonzero_perc',\n",
    "    y='netloc_agg',\n",
    "    order=df.sort_values('nlp_sentences_all_TOP_N_nonzero_perc', ascending=False).netloc_agg,\n",
    "    orient='h',\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "plt.axvline(x=0.75, c='black', ls='--', alpha=0.25)\n",
    "plt.axvline(x=0.50, c='black', ls='--', alpha=0.25)\n",
    "plt.axvline(x=0.25, c='black', ls='--', alpha=0.25)\n",
    "\n",
    "plt.ylabel('Netlocs')\n",
    "plt.xlabel('Percentage of claims with relevance evidence collected')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(15,15))\n",
    "\n",
    "df = textual_entailment_df[['netloc_agg', 'evidence_TE_labels_all_TOP_N']].copy()\n",
    "\n",
    "df['evidence_TE_labels_all_TOP_N_SUP_perc'] = df.evidence_TE_labels_all_TOP_N.apply(\n",
    "    lambda x : 100*len([xx for xx in x if xx == 'SUPPORTS'])/len(x) if x is not None else 0\n",
    ")\n",
    "df['evidence_TE_labels_all_TOP_N_REF_perc'] = df.evidence_TE_labels_all_TOP_N.apply(\n",
    "    lambda x : 100*len([xx for xx in x if xx == 'REFUTES'])/len(x) if x is not None else 0\n",
    ")\n",
    "df['evidence_TE_labels_all_TOP_N_NEI_perc'] = df.evidence_TE_labels_all_TOP_N.apply(\n",
    "    lambda x : 100*len([xx for xx in x if xx == 'NOT ENOUGH INFO'])/len(x) if x is not None else 0\n",
    ")\n",
    "df = df.groupby('netloc_agg').mean().reset_index()\n",
    "df = df.sort_values('evidence_TE_labels_all_TOP_N_NEI_perc', ascending=True).reset_index(drop=True)\n",
    "df_m = df.melt(id_vars='netloc_agg')\n",
    "\n",
    "sns.barplot(\n",
    "    data=df_m,\n",
    "    x='value',\n",
    "    y='netloc_agg',\n",
    "    hue='variable',\n",
    "    order=df.netloc_agg,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "h, l = ax.get_legend_handles_labels()\n",
    "ax.legend(h, ['SUP','REF','NEI'])\n",
    "\n",
    "plt.ylabel('Netlocs')\n",
    "plt.ylabel('Breakdown of mean percentage of evidence labels')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
    "\n",
    "df = textual_entailment_df.copy()\n",
    "\n",
    "\n",
    "sns.barplot(\n",
    "    data=(df.claim_TE_label_weighted_sum_all_TOP_N.value_counts(normalize=True)*100).reset_index(),\n",
    "    x='index',\n",
    "    y='claim_TE_label_weighted_sum_all_TOP_N',\n",
    "    ax=ax[0],\n",
    "    order=['SUPPORTS','REFUTES','NOT ENOUGH INFO']\n",
    ")\n",
    "\n",
    "sns.barplot(\n",
    "    data=(df.claim_TE_label_malon_all_TOP_N.value_counts(normalize=True)*100).reset_index(),\n",
    "    x='index',\n",
    "    y='claim_TE_label_malon_all_TOP_N',\n",
    "    ax=ax[1],\n",
    "    order=['SUPPORTS','REFUTES','NOT ENOUGH INFO']\n",
    ")\n",
    "\n",
    "ax[0].set(ylabel='Percentage of claim labels (WEIGHTED SUM)', xlabel='Labels')\n",
    "ax[1].set(ylabel='Percentage of claim labels (MALON)', xlabel='Labels')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual analysis of selected sentences PLUS textual entailment\n",
    "In addition to quantitative metrics at this step, let's try just looking at some examples by hand.\n",
    "\n",
    "We will, for all netloc_aggs, choose a random sample and check if selected evidence make sense given claim.\n",
    "\n",
    "**Notes**:\n",
    "- The Weighted Sum method provides a better distinction between Refutes and Supports (especifically, it reduces classifying true REFUTE as false SUPPORT from 0.12 to 0.6), but has a harder time with NOT ENOUGH INFO.\n",
    "- The Malon method is the opposite, provides a better distinction between NOT ENOUGH INFO and the other two, but has a higher chance of classifying ture REFUTE as false SUPPORT.\n",
    "- *We should come with a method that mixes both*, as weighted sum considers the evidence scores while malon does not. Perhaps modifying Malon's in a way to consider the positive evidence scores.\n",
    "- Utilising negative evidence scores is not something we do now. So, when it comes to R3 of showing negatively scored evidence, we might have to think on a way to incorporate them in the final label.\n",
    "- Past comments on the manual analysis of selected sentences all fit here.\n",
    "\n",
    "- In some cases, TE will UNLIKELY rate a pair as NEI if the evidence sentence is long. This is because often the TE module will fix favourable (S or R) attention to parts of the information that are not necessarity pertinent to the subject.\n",
    "    - \"Noel Short died in 2001\", for example. The evidences repeatedly talk about 2001 and Short, and in dying and Short, but not all three together. This moves the labels away from NEI, as these sentences are relevant, but not in the exact combination of information that only by imbuing the module with common sense it would be able to spot (like when talking about his wife dying through 'she', the module should know 'she' could not possibly mean Short). In other sentence, it mentions \"Sir Noel Short\" ... \"who has died\" ... \"in the 1970s\", which leads to a REFUTES label, as this long sentence spans lots of information that fool the module.\n",
    "- This might also be an issue when the subject is information-heavy, as the object might get overlooked. Some of these are \"solved\" because they receive very low evidence scores too.\n",
    "    - In the \"The former wash house, Blackness Castle, is located in the administrative territorial entity of Falkirk.\", the evidence \"Address/Name of Site. Former barracks block including outbuildings and yard walls to rear and wash house to northeast, excluding custodian's cottage to northeast and scheduled monument SM90036, Blackness Castle, Blackness LB52456.\" is assigned as SUPPORTS as it cites \"former\", \"wash house\", \"Blackness Castle\", and location names such as \"Address of site\".\n",
    "    - Same with \"Tetradrachm of Elagabalus, Emperor of Rome, from Antioch, Yale University Art Gallery, inv. 1938.6000.104 is an example of an archaeological artifact.\" and the evidence \"Tetradrachm of Elagabalus, Emperor of Rome from Antioch.\"\n",
    "    - Same with \"The Restrictive Trade Practices (Jewellery, Watches and Clocks) Order, 1968 applies to the jurisdiction of Ireland.\"\n",
    "- A similar issue happens when the subject/object is information-heavy on the claim but abbreviated on the evidence.\n",
    "    - Like \"administrative territorial entity of Anaheim\" becoming just \"Anaheim\".\n",
    "- Obviously there are issues when whatever is said on the sentence needs some common sense/knowledge to conclude the claim. Like in the \"The former wash house, Blackness Castle, is located in the administrative territorial entity of Falkirk.\" case, the evidence saying\n",
    "    - For instance, \"A Rocky Shore, with the Tour de Croy in the Distance is located in Tate.\" is true because the link leads to the Tate website with the art piece on it, not because it particularly says so in its text.\n",
    "    - \"Naravoor Lps is located in India.\" needs knowledge that Kannur is in India.\n",
    "- Rare ways of writing information throws the modules out\n",
    "    - Like \"Deathdate\" instead of \"Death date\", on 'Alec Waugh died on 03/09/1981.' with evidence \"Birthdate: 8 July 1898. Death date: 3 September 1981.\"\n",
    "\n",
    "To alter before crowdsourcing:\n",
    "- **Remember** to alter the URLs leading to paywall-protected pages by using their https://archive.ph/ version.\n",
    "    - You can build a list by checking if 'archive.ph' is in the html.\n",
    "- A handful of websites are not in English, we can get a list from netloc archinform.net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textual_entailment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = textual_entailment_df.copy()\n",
    "\n",
    "try:\n",
    "    for netloc in df.netloc_agg.unique().tolist():\n",
    "        print('-'*10)\n",
    "        print(netloc,'\\n')\n",
    "        sampled_rows = df[df['netloc_agg'] == netloc].sample(3, random_state=42)\n",
    "        for i, row in sampled_rows.iterrows():\n",
    "            print(\n",
    "                '->', row['final_verbalisation'],'\\n',\n",
    "                '\\t-WS:', row['claim_TE_label_weighted_sum_all_TOP_N'], f\"({row['claim_TE_prob_weighted_sum_all_TOP_N']})\\n\",\n",
    "                '\\t-M: ', row['claim_TE_label_malon_all_TOP_N']\n",
    "            )\n",
    "            print('->>', row['final_url'])\n",
    "            for i, sentence in enumerate(row['nlp_sentences_all_TOP_N']):\n",
    "                if sentence['score'] > SCORE_THRESHOLD:\n",
    "                    print(\n",
    "                        f\"\\t{i+1}. {sentence['sentence']}\\n\",\n",
    "                        f\"\\t-Evidence Score: {sentence['score']}\\n\",\n",
    "                        f\"\\t-Label Prob: {row['evidence_TE_prob_all_TOP_N'][i]}\\n\",\n",
    "                        f\"\\t-Label Prob Weighted: {row['evidence_TE_prob_weighted_all_TOP_N'][i]}\\n\",\n",
    "                        f\"\\t-Label: {row['evidence_TE_labels_all_TOP_N'][i]}\\n\",\n",
    "                    )\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"\\t{i+1}. {sentence['sentence']}\\n\",\n",
    "                        f\"\\t-Evidence Score: {sentence['score']}\\n\",\n",
    "                        f\"\\t-Label Prob: {row['evidence_TE_prob_all_TOP_N'][i]}\\n\",\n",
    "                        f\"\\t-Label Prob Weighted: {[0,0,0]}\\n\",\n",
    "                        f\"\\t-Label: {row['evidence_TE_labels_all_TOP_N'][i]}\\n\",\n",
    "                    )\n",
    "            print()\n",
    "except Exception:\n",
    "    print(row)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
